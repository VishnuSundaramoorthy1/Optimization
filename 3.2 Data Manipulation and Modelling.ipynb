{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cb94f86-dc6e-47ea-87c0-b7cca192bfe3",
   "metadata": {},
   "source": [
    "## Parameters that can be changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6cf4a38e-28a2-4792-b058-5136dbe870e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_file_name = \"Sales_History_Apodaca.csv\"\n",
    "raw_data_file_sheet_name = 'Sales_History_Apodaca'\n",
    "filter_date = '2023-02-11'\n",
    "\n",
    "logistic_data_file_name = 'US CAN ZIP CODE FILE 1.xlsx'\n",
    "logistic_data_sheet_name = 'us can zip'\n",
    "\n",
    "FG_health_file_name = 'FG_Health_Apodaca.csv'\n",
    "\n",
    "# Coordinates of Warehouses PHR --> McAllen, Texas; FDW --> Florence, Kentucky; IDW --> Indianapolis, AW --> Ontario, California\n",
    "Warehouse_Coordinate = {\n",
    "    'PHR': (26.1570071, -98.321983),\n",
    "    'FDW': (38.9860247, -84.6139836),\n",
    "    'IDW': (39.7332942, -86.3482675),\n",
    "    'AW': (34.0434872, -117.5760558)\n",
    "}\n",
    "\n",
    "warehouse_lead_time ={\n",
    "    'PHR' : 3,\n",
    "    'FDW' : 7,\n",
    "    'IDW' : 7,\n",
    "    'AW' : 7\n",
    "}\n",
    "\n",
    "warehouse_name = ['PHR', 'FDW', 'IDW', 'AW']\n",
    "\n",
    "warehouse_to_consider = ['FDW', 'IDW', 'PHR', 'AW']\n",
    "\n",
    "current_date = '2025-02-17'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "64e5ee09-a158-4e91-9cb2-634d0c98f82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required packages\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from geopy.distance import geodesic\n",
    "from scipy.stats import norm\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2f6c301d-9b2f-4398-afee-da59bb2f88f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the CSV file\n",
    "dtype_column = {'Material Number': str,\n",
    "                'Ship-To Postal Code': str}\n",
    "df = pd.read_csv(raw_data_file_name, dtype=dtype_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "575b62b2-af89-4529-aa3d-8e489d3836b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Billing Date'] = pd.to_datetime(df['Billing Date'])\n",
    "df['Billing Quantity in Base Unit'] = pd.to_numeric(df['Billing Quantity in Base Unit'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d748b5b5-39a7-4d25-8be7-2eba6799bd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the unwanted columns that are not named\n",
    "df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "df = df.iloc[:-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "96e3ca05-448d-423c-88f1-0b696b8b2f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Unique postal codes: 4212\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of Unique postal codes: {df['Ship-To Postal Code'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5eba10bf-b017-4d2f-a4cc-4277cc7b1757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting only the first 5 characters in the 'Ship to Postal Code' column\n",
    "df['Ship-To Postal Code'] = df['Ship-To Postal Code'].str[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630b33cc-53fe-4370-8e33-f2a5508fbcae",
   "metadata": {},
   "source": [
    "## Reading complete sales data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b8ec49e2-a0b5-49ec-bbf9-1bbbacaf34c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the CSV file\n",
    "dtype_column_1 = {'Material Number': str,\n",
    "                'Ship-To Postal Code': str\n",
    "                 }\n",
    "\n",
    "# Define a converter function to remove commas and handle empty strings\n",
    "def remove_commas(x):\n",
    "    if x == '':\n",
    "        return np.nan\n",
    "    return float(x.replace(',', ''))\n",
    "\n",
    "# Specify the converters for columns that need to be numeric\n",
    "converters = {\n",
    "    'Billing Quantity in Base Unit': remove_commas\n",
    "}\n",
    "\n",
    "# Read the CSV file with specified data types and converters\n",
    "df1 = pd.read_csv(\"Sales_History_Apodaca_All.csv\", dtype=dtype_column_1, converters=converters)\n",
    "\n",
    "df1 = df1.iloc[:-3]\n",
    "df1 = df1.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a05e1cc8-5a49-4047-a586-b83d96025e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['Ship-To Postal Code'])\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c6d0b075-72b3-4a90-be67-9a28eabf9c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 115005 entries, 0 to 115004\n",
      "Data columns (total 6 columns):\n",
      " #   Column                         Non-Null Count   Dtype  \n",
      "---  ------                         --------------   -----  \n",
      " 0   Material Number                115005 non-null  object \n",
      " 1   Ship-To Postal Code            115005 non-null  object \n",
      " 2   Ship-To Customer City          115005 non-null  object \n",
      " 3   Ship-To Region                 115005 non-null  object \n",
      " 4   Billing Quantity in Base Unit  114895 non-null  float64\n",
      " 5   Shipping Plant                 115005 non-null  object \n",
      "dtypes: float64(1), object(5)\n",
      "memory usage: 5.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df_filter = df[['Material Number', \n",
    "                'Ship-To Postal Code', \n",
    "                'Ship-To Customer City', \n",
    "                'Ship-To Region', \n",
    "                'Billing Quantity in Base Unit', \n",
    "                'Shipping Plant']]\n",
    "df_filter.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "229f0af1-9f1c-4e62-b67e-85b0802f6acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\610168286\\AppData\\Local\\Temp\\ipykernel_21056\\1888360844.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_filter['Billing Quantity in Base Unit'] = pd.to_numeric(df_filter['Billing Quantity in Base Unit'], errors='coerce')\n"
     ]
    }
   ],
   "source": [
    "df_filter['Billing Quantity in Base Unit'] = pd.to_numeric(df_filter['Billing Quantity in Base Unit'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "157b31c6-9213-4d1c-a80a-18612a211940",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_group = df_filter.groupby(['Material Number', \n",
    "                              'Ship-To Postal Code', \n",
    "                              'Ship-To Customer City', \n",
    "                              'Ship-To Region',\n",
    "                              'Shipping Plant']).agg({'Billing Quantity in Base Unit': 'sum'}).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a100fb4-eb94-41aa-a589-fcb90b91f39a",
   "metadata": {},
   "source": [
    "### Loading the Logsitic Zip code data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4d32186a-4504-4d28-82ef-75354f9f471b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the Zip code data that has Lat & long details based on Zip code\n",
    "df_logistic = pd.read_excel(logistic_data_file_name, sheet_name= logistic_data_sheet_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3d77110d-74b9-4fcf-be42-d893558c5968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering only US Zip code data\n",
    "df_logistic.drop(df_logistic[df_logistic['COUNTRY'] != 'US'].index, inplace =True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d16497e-35ae-444d-a615-79f600e8676d",
   "metadata": {},
   "source": [
    "### Merging Sales data & Logistics data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bdb32987-4eb9-4d35-940c-bdabefaf0a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the two DataFrames using left join\n",
    "df_group = pd.merge(df_group, df_logistic[['zip'] + ['LAT', 'LON']],\n",
    "             left_on = 'Ship-To Postal Code',\n",
    "             right_on = 'zip',\n",
    "             how = 'left')\n",
    "\n",
    "\n",
    "df_group.drop(columns= ['zip'], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40463c8-4bf1-4cff-920d-dda74a9b9d65",
   "metadata": {},
   "source": [
    "## Distance Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "137019ed-858e-4670-8c76-27ed1aa1efb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the distance\n",
    "\n",
    "def calculate_distance(row, j):\n",
    "    if pd.notnull(row['LAT']) and pd.notnull(row['LON']):\n",
    "        return geodesic((row['LAT'], row['LON']), j).miles\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "for i, j in Warehouse_Coordinate.items():\n",
    "    if i in warehouse_to_consider:\n",
    "        df_group[i] = df_group.apply(lambda row: calculate_distance(row, j), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "54067a71-f2da-4f5c-9b83-956c641c4aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\610168286\\AppData\\Local\\Temp\\ipykernel_21056\\3713160561.py:1: FutureWarning: The behavior of DataFrame.idxmin with all-NA values, or any-NA and skipna=False, is deprecated. In a future version this will raise ValueError\n",
      "  df_group['Nearest Warehouse'] = df_group[warehouse_to_consider].idxmin(axis = 1)\n"
     ]
    }
   ],
   "source": [
    "df_group['Nearest Warehouse'] = df_group[warehouse_to_consider].idxmin(axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd2c018-d6ad-4d21-a3c6-3ee45d0b5998",
   "metadata": {},
   "source": [
    "### FG Health Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e23e514c-5b1a-4550-9d9a-ae154c183e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\610168286\\AppData\\Local\\Temp\\ipykernel_21056\\1111916359.py:3: DtypeWarning: Columns (19,20,21,65,69,70,90,111,112,148) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_FG = pd.read_csv(FG_health_file_name, dtype=dtype_column)\n"
     ]
    }
   ],
   "source": [
    "dtype_column = {'Material Number': str,\n",
    "                'SS > 0': str}\n",
    "df_FG = pd.read_csv(FG_health_file_name, dtype=dtype_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6ed3b7-a3da-416f-9875-ef50f71f55dd",
   "metadata": {},
   "source": [
    "### Merging FG Health"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2a66924d-f3fc-42f2-b645-7c11e34472c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging FG health to filter out part numbers without safety stock\n",
    "df_group = pd.merge(df_group, df_FG[['Material Number', 'SS > 0']],\n",
    "                   on = 'Material Number',\n",
    "                   how = 'left')\n",
    "df_group = df_group.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "21b0112c-7c26-4cc2-af6a-51be0336e4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_group = df_group[df_group['SS > 0'] != 'N']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c198af-b577-45f5-a72a-c2f60dafd927",
   "metadata": {},
   "source": [
    "## Customer Concentration Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "dca8fcba-e462-485e-86bd-f6c893c4120e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def customer_concentration(df, df_name):\n",
    "    # Grouping data to find the total billing quantity for each part number based on nearest warehouse\n",
    "    \n",
    "    part_warehouse_demand = df.groupby(['Material Number', 'Nearest Warehouse'])['Billing Quantity in Base Unit'].sum().reset_index()\n",
    "    part_warehouse_demand.rename(columns={'Billing Quantity in Base Unit' : 'Demand Per Warehouse'}, inplace = True)\n",
    "    \n",
    "    # Grouping data to find the total billing quantity for each part number\n",
    "    total_part_demand = df.groupby('Material Number')['Billing Quantity in Base Unit'].sum().reset_index()\n",
    "    total_part_demand.rename(columns={'Billing Quantity in Base Unit' : 'Total Demand'}, inplace = True)\n",
    "\n",
    "    df_merge = pd.merge(part_warehouse_demand, total_part_demand,\n",
    "                   on = 'Material Number')\n",
    "    '''\n",
    "    # Converting the datatype to numeric\n",
    "    df_merge['Demand Per Warehouse'] = pd.to_numeric(df_merge['Demand Per Warehouse'], errors='coerce')\n",
    "    df_merge['Total Demand'] = pd.to_numeric(df_merge['Total Demand'], errors='coerce')\n",
    "    '''\n",
    "    \n",
    "\n",
    "    df_merge['% Concentration'] = ((df_merge['Demand Per Warehouse']/df_merge['Total Demand']) * 100).round()\n",
    "\n",
    "    # Dynamically name the resulting DataFrame\n",
    "    globals()[df_name] = df_merge\n",
    "    \n",
    "    return df_merge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "982678f4-e29d-40c8-8fd9-ee7440e67238",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_group = customer_concentration(df_group, 'df_group_merge')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e826ee5c-4046-415a-95fb-d321361af3df",
   "metadata": {},
   "source": [
    "## Pivoting the Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9a5013b2-6c4c-45ec-810f-630609a087f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pivot_dataframe(df):\n",
    "    # Pivoting the df_merge DataFrame to find the concentration of parts demand on warehouse\n",
    "    df_pivot = df.pivot_table(index = ['Material Number', 'Total Demand'],\n",
    "                                   columns = 'Nearest Warehouse',\n",
    "                                   values = '% Concentration').reset_index()\n",
    "    df_pivot = df_pivot.fillna(0)\n",
    "    df_pivot.columns.name = None\n",
    "    df_pivot.reset_index(drop = True, inplace = True)\n",
    "\n",
    "    # Dynamically name the resulting DataFrame\n",
    "    #globals()[df_name] = df_pivot\n",
    "\n",
    "    rename_dict = {warehouse: f'% {warehouse}' for warehouse in warehouse_to_consider}\n",
    "    df_pivot.rename(columns=rename_dict, inplace = True)\n",
    "    \n",
    "    return df_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "16bc04e4-2a2d-49dd-9521-4581e4319878",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_group_pivot = pivot_dataframe(df_group_merge)\n",
    "\n",
    "df_group_concentration = df_group_pivot.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "371bebb8-ef6c-41a1-bdcb-b49b862d6d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This lead time is calculated based on source location to warehouse\n",
    "\n",
    "for warehouse in warehouse_to_consider:\n",
    "    df_group_concentration[f'{warehouse} Lead Time'] = warehouse_lead_time[warehouse]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d433e30-7fca-47c1-9c09-949ea86f4851",
   "metadata": {},
   "source": [
    "## Creating new DataFrame to calculate Standard deviation & Average usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1f62b2f3-e421-48d8-a219-548c9a0c88bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating this DataFrame to have a column with both part number and date, so as to merge later\n",
    "\n",
    "df_std_copy = df1.copy()\n",
    "df_std_copy = df_std_copy[['Material Number', 'Billing Quantity in Base Unit' , 'Billing Date']]\n",
    "df_std_copy['Billing Date'] = pd.to_datetime(df_std_copy['Billing Date'], errors = 'coerce')\n",
    "df_std_copy['Merge Name'] = df_std_copy['Material Number'] + \" \" + df_std_copy['Billing Date'].dt.strftime('%Y-%m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2d4ec3cd-b8f2-4b5a-8127-8b091d57a447",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_std = df1.copy()\n",
    "df_std = df_std[['Material Number', 'Billing Quantity in Base Unit' , 'Billing Date']]\n",
    "df_std.set_index('Billing Date', inplace = True)\n",
    "df_std = df_std.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2ad6b8c7-e739-44ed-9d33-49f713e3e2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating this DataFrame to have 24 months slots for every part number to calculate Standard Deviation & Avergae\n",
    "\n",
    "# Create a date range for the past 24 months up to December 2024\n",
    "end_date = pd.to_datetime(current_date)\n",
    "start_date = end_date - pd.DateOffset(months=23)\n",
    "date_range = pd.date_range(start=start_date, end=end_date, freq='MS')\n",
    "\n",
    "# Create a DataFrame with all combinations of 'Manufacturing Material Number' and the date range\n",
    "part_numbers = df_std_copy['Material Number'].unique()\n",
    "all_combinations = pd.MultiIndex.from_product([part_numbers, date_range], names=['Material Number', 'Billing Date'])\n",
    "all_combinations_df = pd.DataFrame(index=all_combinations).reset_index()\n",
    "\n",
    "all_combinations_df['Merge Name'] = all_combinations_df['Material Number'] + \" \" + all_combinations_df['Billing Date'].dt.strftime('%Y-%m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b9081c6f-e9ff-4522-be10-7829383e8892",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\610168286\\AppData\\Local\\Temp\\ipykernel_21056\\4284634732.py:8: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n",
      "  monthly_demand = df_std.groupby(['Material Number', pd.Grouper(key='Billing Date', freq='M')])['Billing Quantity in Base Unit'].sum().fillna(0).reset_index()\n"
     ]
    }
   ],
   "source": [
    "# Creating this DataFrame to group billing quantity for each part number based on month\n",
    "\n",
    "# Convert 'Billing Date' to datetime format\n",
    "df_std['Billing Date'] = pd.to_datetime(df_std['Billing Date'], errors='coerce')\n",
    "\n",
    "# Convert 'Billing Quantity in Base Unit' to numeric\n",
    "df_std['Billing Quantity in Base Unit'] = pd.to_numeric(df_std['Billing Quantity in Base Unit'], errors='coerce')\n",
    "monthly_demand = df_std.groupby(['Material Number', pd.Grouper(key='Billing Date', freq='M')])['Billing Quantity in Base Unit'].sum().fillna(0).reset_index()\n",
    "\n",
    "monthly_demand['Merge Name'] = monthly_demand['Material Number'] + \" \" + monthly_demand['Billing Date'].dt.strftime('%Y-%m')\n",
    "monthly_demand = monthly_demand.reindex(columns=['Material Number', 'Billing Date', 'Merge Name', 'Billing Quantity in Base Unit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "89b6940a-0c20-43e4-99ef-c544f6a38866",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demand = pd.merge(all_combinations_df, monthly_demand[['Merge Name', 'Billing Quantity in Base Unit']],\n",
    "                    on = 'Merge Name',\n",
    "                    how = 'left')\n",
    "\n",
    "df_demand['Billing Quantity in Base Unit'] = df_demand['Billing Quantity in Base Unit'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7ce6065b-92d9-40c9-a2b2-3b8de9ade765",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_dev_per_part = df_demand.groupby('Material Number')['Billing Quantity in Base Unit'].std()\n",
    "avg_per_part = df_demand.groupby('Material Number')['Billing Quantity in Base Unit'].mean()\n",
    "\n",
    "\n",
    "Standard_Deviation = pd.DataFrame(std_dev_per_part).reset_index()\n",
    "Average_Usage = pd.DataFrame(avg_per_part).reset_index()\n",
    "\n",
    "Standard_Deviation = Standard_Deviation.rename(columns={'Billing Quantity in Base Unit' : 'Standard Deviation'})\n",
    "Average_Usage = Average_Usage.rename(columns={'Billing Quantity in Base Unit' : 'Average Usage'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8669c024-7426-495a-b826-c40eb3600b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_group_concentration = pd.merge(df_group_concentration, Average_Usage[['Material Number', 'Average Usage']],\n",
    "                                            on = 'Material Number',\n",
    "                                            how = 'left')\n",
    "\n",
    "df_group_concentration = pd.merge(df_group_concentration, Standard_Deviation[['Material Number', 'Standard Deviation']],\n",
    "                                            on = 'Material Number',\n",
    "                                            how = 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8486e1d-774c-4663-b5dd-ee94f7d52466",
   "metadata": {},
   "source": [
    "## Safety Stock Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3e923107-da4b-426f-a87a-437f2024b901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formula for Safety stock calculation ---> Zfactor * standard deviation * sqrt(Average demand over lead time)\n",
    "\n",
    "service_level = 0.95\n",
    "z_score = norm.ppf(service_level).round(2)\n",
    "\n",
    "# Function to calculate safety stock\n",
    "def calculate_safety_stock(row, z_score, lead_time_col, std_dev_col):\n",
    "    return z_score * row[std_dev_col] * np.sqrt(row[lead_time_col]/30)\n",
    "\n",
    "warehouse_name = warehouse_to_consider\n",
    "\n",
    "# Function to calculate safety stock for a given DataFrame\n",
    "def calculate_safety_stock_for_df(df):\n",
    "    for index, row in df.iterrows():\n",
    "       for i in warehouse_name:\n",
    "           lead_time_col = f'{i} Lead Time'\n",
    "           std_dev_col = 'Standard Deviation'\n",
    "           safety_stock_col = f'{i} Safety Stock'\n",
    "           safety_stock = calculate_safety_stock(row, z_score, lead_time_col, std_dev_col)\n",
    "           \n",
    "           df.at[index, safety_stock_col] = safety_stock\n",
    "    return df\n",
    "    \n",
    "\n",
    "# Calculate safety stock for both DataFrames\n",
    "df_group_concentration = calculate_safety_stock_for_df(df_group_concentration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7b25c8-1327-4007-a834-e9b5035e2d3f",
   "metadata": {},
   "source": [
    "## Average Demand over leadtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "cde2a999-d552-49ae-bddc-dd1250a0115a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formula for Average Demand over lead time ---> lead time (months) * average\n",
    "\n",
    "def calculate_average_demand(row, average, leadtime):\n",
    "    return row[average] * (row[leadtime] / 30)\n",
    "\n",
    "warehouse_name = warehouse_to_consider\n",
    "\n",
    "\n",
    "# Function to calculate average demand for a given DataFrame\n",
    "def calculate_average_demand_for_df(df):\n",
    "    for index, row in df.iterrows():\n",
    "        for i in warehouse_to_consider:\n",
    "            average = 'Average Usage'\n",
    "            leadtime = f'{i} Lead Time'\n",
    "            average_demand = f'{i} Average'\n",
    "            \n",
    "            # Calculate average demand for the current row and warehouse\n",
    "            avg_demand = calculate_average_demand(row, average, leadtime)\n",
    "            \n",
    "            # Assign the calculated average demand to the DataFrame\n",
    "            df.at[index, average_demand] = avg_demand\n",
    "    return df\n",
    "\n",
    "# Calculate avergae demand over lead time for both DataFrames\n",
    "df_group_concentration = calculate_average_demand_for_df(df_group_concentration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76120ec8-ca89-4cff-9437-767f2e280810",
   "metadata": {},
   "source": [
    "## Reorder Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "30930c99-12ec-4308-926d-14d8244eb959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formula for Average Demand over lead time ---> lead time (months) * average\n",
    "\n",
    "def calculate_reorder_point(row, safety_stock, average):\n",
    "    return row[safety_stock] + row[average]\n",
    "\n",
    "warehouse_name = warehouse_to_consider\n",
    "\n",
    "# Function to calculate reorder point for a given DataFrame\n",
    "def calculate_reorder_point_for_df(df):\n",
    "    for index, row in df.iterrows():\n",
    "        for i in warehouse_to_consider:\n",
    "            average = f'{i} Average'\n",
    "            safety_stock = f'{i} Safety Stock'\n",
    "            reorder_point = f'{i} ROP'\n",
    "            \n",
    "            # Calculate reorder point for the current row and warehouse\n",
    "            rop = calculate_reorder_point(row, safety_stock, average)\n",
    "            \n",
    "            # Assign the calculated reorder point to the DataFrame\n",
    "            df.at[index, reorder_point] = round(rop)\n",
    "    return df\n",
    "\n",
    "# Calculate reorder point for both DataFrames\n",
    "df_group_concentration = calculate_reorder_point_for_df(df_group_concentration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "64ec46a5-b138-435f-a727-8f9be85ebf78",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_to_drop = ['Standard Deviation', 'Average Usage']\n",
    "\n",
    "for warehouse in warehouse_to_consider:\n",
    "    column_to_drop.append(f'{warehouse} Safety Stock')\n",
    "    column_to_drop.append(f'{warehouse} Average')\n",
    "    \n",
    "df_final_group = df_group_concentration.drop(columns=column_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a3a56200-ac45-4597-ab23-0aa03cc4ec6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_demand.to_excel(\"Not to use - demand try.xlsx\")\n",
    "#df_final_within_900.to_csv('Data_For_Julia_within_900.csv', index = False)\n",
    "#df_final_commom.to_csv('Data_For_Julia_common.csv', index = False)\n",
    "#df_merge_group.to_csv('Not to use - Data for Julia.csv', index = False)\n",
    "#df_group.to_excel('Not to use - df_group.xlsx', index = False)\n",
    "#df_concentration.to_excel('Final GUA Produced parts warehouse.xlsx', index = False)\n",
    "df_final_group.to_csv('Data_For_Julia_10.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
